sites: [localhost]
# sites can be set to a comma separated list of remote sites you have access to
# eg.: sites: [stampede] or sites: [stampede, blacklight]
# The sites definition in the config can be overridden by specifying -sites on the
# swift command line as follows : swift -sites stampede p4.swift

# Default site for examples 1-3
# This site runs tasks on the local machine
site.localhost {
     execution {
        type    : "local"                            # Execution is local
        URL     : "localhost"                        # Point at localhost to run locally
     }
     staging             : direct                     # Files are on the same machine, so can be accessed "directly"
     workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Directory where work is done
     maxParallelTasks    : 101                         # Maximum number of parallel tasks
     initialParallelTasks: 100                         # Maximum number of tasks at start
     app.ALL { executable: "*" }                       # All tasks to be found from commandline
}


# Instructions for Stampede
# 1. If you are running on the stampede login nodes set jobManager: "local:slurm"
site.stampede {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "stampede.tacc.xsede.org"         # Stampede login nodes login[1..4].stampede.tacc.utexas.edu
        jobManager: "ssh-cl:slurm"                    # Use ssh-cl to connect, slurm is the Local resource manager
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 16                       # Tasks per Node
            jobQueue        : "development"           # Select queue from (development, normal, large)
            maxJobTime      : "00:25:00"              # Time requested per job
        }
    }
    staging             : "local"                     # Stage files from "local" system
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Stampede, with XSEDE tutorial reservation
# 1. If you are running on the stampede login nodes set jobManager: "local:slurm"
site.rstampede {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "stampede.tacc.xsede.org"         # Stampede login nodes login[1..4].stampede.tacc.utexas.edu
        jobManager: "ssh-cl:slurm"                    # Use ssh-cl to connect, slurm is the Local resource manager
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 16                       # Tasks per Node
            jobQueue        : "development"           # Select queue from (development, normal, large)
            maxJobTime      : "00:25:00"              # Time requested per job
            jobOptions.slurm{
                "reservation" : "XSEDE_2016_1"
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Blacklight
# 1. If you are running on the blacklight login nodes, set jobManager: "local:pbs"
# 2. If you are running Set userHomeOverride : "/lustre/blacklight2/YOUR_USERNAME_ON_BLACKLIGHT/swiftwork"
# 4. Set workDirectory : "/tmp/YOUR_USERNAME_ON_BLACKLIGHT/swiftwork"
site.blacklight {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "blacklight.psc.xsede.org"        # Blacklight login URL
        jobManager: "ssh-cl:pbs"                      # use ssh-cl to connect, pbs is the Local Resource manager(LRM)
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 16                       # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
    	    jobQueue        : "debug"
            jobOptions {
                ppn         : "16"                    # Virtual processors per node per Job
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system to Blacklight
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Blacklight with XSEDE reserved queues
# 1. If you are running on the blacklight login nodes, set jobManager: "local:pbs"
# 2. If you are running Set userHomeOverride : "/lustre/blacklight2/YOUR_USERNAME_ON_BLACKLIGHT/swiftwork"
# 4. Set workDirectory : "/tmp/YOUR_USERNAME_ON_BLACKLIGHT/swiftwork"
site.rblacklight {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "blacklight.psc.xsede.org"        # Blacklight login URL
        jobManager: "ssh-cl:pbs"                      # use ssh-cl to connect, pbs is the Local Resource manager(LRM)
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 16                       # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
    	    jobQueue        : "res_1"
            jobOptions {
                ppn         : "16"                    # Virtual processors per node per Job
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system to Blacklight
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Gordon
# 1. Do *NOT* run on the Gordon login nodes. There are memory limits which prevent swift from running
#    properly on these machines.
site.gordon {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "gordon.sdsc.xsede.org"           # Gordon login URL
        jobManager: "ssh-cl:pbs"                      # use ssh-cl to connect, pbs is the Local Resource manager(LRM)
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 16                       # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
	    jobQueue        : "normal"
            jobOptions {
                ppn         : "16"                    # Virtual processors per node per Job
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system to Gordon
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Configuration for Comet - running on a slurm compute node, submitting back to Comet

#  Swift will not run on Comet login nodes due to memory limts

site.comet {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "comet.sdsc.xsede.org"            # Comet login URL - not used for local:slurm
        jobManager: "local:slurm"                     # use slurm commands to submit jobs locally
        options {
            maxJobs         : 4                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 24                      # Tasks per Node
            maxJobTime      : "00:30:00"              # Time requested per job
	    jobQueue        : "compute"               # Submit to compute partition (from shared)
        }
    }
    staging             : "local"                     # Stage files from "local" fs to compute nodes
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Work dir on compute nodes
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All apps to be found from commandline
}

# Configuration for Bridges

site.bridges {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "bridges.psc.xsede.org"            # Comet login URL - not used for local:slurm
        jobManager: "local:slurm"                     # use slurm commands to submit jobs locally
        options {
            maxJobs         : 4                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 28                     # Tasks per Node
            maxJobTime      : "00:30:00"              # Time requested per job
	    jobQueue        : "RM"               # Submit to compute partition (from shared)
        }
    }
    staging             : "local"                     # Stage files from "local" fs to compute nodes
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Work dir on compute nodes
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All apps to be found from commandline
}

# Configuration for Comet MPI - 6 app tasks x 4 cores per node

site.comet6 {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "comet.sdsc.xsede.org"            # Comet login URL - not used for local:slurm
        jobManager: "local:slurm"                     # use slurm commands to submit jobs locally
        options {
            maxJobs         : 4                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 6                       # Tasks per Node
            maxJobTime      : "00:30:00"              # Time requested per job
	    jobQueue        : "compute"               # Submit to compute partition (from shared)
        }
    }
    staging             : "local"                     # Stage files from "local" fs to compute nodes
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Work dir on compute nodes
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All apps to be found from commandline
}

# Configuration for Comet MPI - 6 app tasks x 4 cores per node

site.comet6 {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "comet.sdsc.xsede.org"            # Comet login URL - not used for local:slurm
        jobManager: "local:slurm"                     # use slurm commands to submit jobs locally
        options {
            maxJobs         : 4                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 6                       # Tasks per Node
            maxJobTime      : "00:30:00"              # Time requested per job
	    jobQueue        : "compute"               # Submit to compute partition (from shared)
        }
    }
    staging             : "local"                     # Stage files from "local" fs to compute nodes
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Work dir on compute nodes
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All apps to be found from commandline
}

# Configuration for Comet with XSEDE Tutorial Reservation

site.rcomet {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "comet.sdsc.xsede.org"            # Comet login URL - not used for local:slurm
        jobManager: "local:slurm"                     # use slurm commands to submit jobs locally
        options {
            maxJobs         : 4                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 24                      # Tasks per Node
            maxJobTime      : "00:30:00"              # Time requested per job
            jobOptions.slurm{
                "reservation" : "XSEDE16Mats"
            }
	    jobQueue        : "compute"               # Submit to compute partition (from shared)
        }
    }
    staging             : "local"                     # Stage files from "local" fs to compute nodes
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Work dir on compute nodes
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All apps to be found from commandline
}

site.rcomet6 {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "comet.sdsc.xsede.org"            # Comet login URL - not used for local:slurm
        jobManager: "local:slurm"                     # use slurm commands to submit jobs locally
        options {
            maxJobs         : 4                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 6                       # Tasks per Node
            maxJobTime      : "00:30:00"              # Time requested per job
            jobOptions.slurm{
                "reservation" : "XSEDE16Mats"
            }
	    jobQueue        : "compute"               # Submit to compute partition (from shared)
        }
    }
    staging             : "local"                     # Stage files from "local" fs to compute nodes
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Work dir on compute nodes
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All apps to be found from commandline
}

# Instructions for Trestles
# 1. Do *NOT* run on the Trestles login nodes. There are memory limits which prevent swift from running
#    properly on these machines.
site.trestles {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "trestles.sdsc.edu"                 # Trestles login URL
        jobManager: "ssh-cl:pbs"                      # use ssh-cl to connect, pbs is the Local Resource manager(LRM)
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 4                       # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
            jobOptions {
                ppn         : "16"                    # Virtual processors per node per Job
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system to Trestles
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Stampede MPI configurations
site.stampede-mpi {
    execution {
        type      : "coaster"                        # Use coasters to run on remote sites
        URL       : "login4.stampede.tacc.utexas.edu" # Stampede login nodes login[1..4].stampede.tacc.utexas.edu
        jobManager: "local:slurm"                    # Use ssh-cl to connect, slurm is the Local resource manager
        options {
            maxJobs         : 2                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 4                       # Tasks per Node
            jobQueue        : "normal"                # Select queue from (development, normal, large)
            maxJobTime      : "00:25:00"              # Time requested per job
        }
    }
    staging             : "direct"                    # Access files directly from a shared filesystem
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}


TCPPortRange: "50000,51000"                           # TCP port range used by swift to communicate with remote sites
lazyErrors: false                                     # Swift fails immediately upon encountering an error
executionRetries: 0                                   # Set number of retries upon task failures
keepSiteDir: true                                     # Keep Site Dir (useful for debug)
providerStagingPinSwiftFiles: false                   # Pin staging files (useful for debug)
alwaysTransferWrapperLog: true                        # Transfer wrapper logs (useful for debug)

